---
editor_options:
  markdown:
    wrap: 72
---

------------------------------------------------------------------------

# Module-1 Assignment by "Sankalp Biswal"

# 2024-01-13

# Introduction:

The Ames Housing dataset offers a comprehensive analysis of residential
homes in Ames, Iowa, from 2006 to 2010. In this study, we aimed to
understand the factors influencing house prices and develop a predictive
model for SalePrice, the property's sale price. Through Exploratory Data
Analysis (EDA), we summarized the dataset's characteristics, addressed
missing values, and examined correlations among variables. We plotted
the distribution of SalePrice to understand its skewness and identify
outliers. A rigorous statistical analysis was performed using
correlation matrices and scatter plots to identify relationships between
SalePrice and other features. We then proceeded to fit a linear
regression model, considering multicollinearity and leveraging the
Variance Inflation Factor (VIF) to refine our predictors. The model's
diagnostics were conducted using residual and leverage plots to detect
outliers and influential points. By employing all-subsets regression, we
identified the "best" model that balances model complexity with
predictive power. This comprehensive analysis leads us to a conclusion
that synthesizes our findings and provides actionable insights.

# Importing Libraries

```{r}
library(dplyr)
library(ggplot2)
library(corrplot)
library(RColorBrewer)
library(car)
library(ggcorrplot)
library(tidyr)
library(leaps)
```

### Q1. Load the Ames housing dataset.

A1.

```{r}
df <- read.csv("AmesHousing.csv")
df<- df %>%
  select(-Order, -PID)


```

### Q2. Perform Exploratory Data Analysis and use descriptive statistics to describe the data.

A2.

1.  We've used `summary()` for descriptive statistics

```{r}
summary(df)
```

-   We've obtained descriptive statistics for our dataset and we can see
    that few variables have "NA" or "missing values" which we'll deal
    with later in the assignment.

2.  Plotting a histogram for "SalePrice"

```{r}
options(scipen = 10)

hist(df$SalePrice, main = "Distribution of Sale Price", xlab = "Sale Price")

```

-   We observe that our response variable "***SalePrice"*** follows a
    normal distribution.

3.  Plotting boxplot for ***"SalePrice"***

```{r}

# Boxplot of Sale Price
boxplot(df$SalePrice, main = "Boxplot of Sale Price")



```

-   **Interpreting this boxplot for "Sale Price":**
    -   The distribution of sale prices is right-skewed, as evidenced by
        the median being closer to the bottom of the box and the long
        whisker extending upward with several outliers. This suggests
        that while the bulk of the sale prices are clustered at the
        lower end, there are quite a few houses that sold for much
        higher prices.

    -   The presence of outliers indicates that there are some houses
        with sale prices significantly higher than typical values in the
        dataset. This could be due to a variety of factors such as
        location, size, or market conditions at the time of sale.

    -   There are no outliers on the low end, suggesting that there are
        not many houses selling for exceptionally low prices relative to
        the rest of the data.

### Q3. Prepare the dataset for modeling by imputing missing values with the variable's mean value or any other value that you prefer

A3.

1.  We'll impute any missing values in the dataset with the respective
    variable mean. (OpenAI, 2023)

    ```{r}
    # Impute missing values in numeric variables with mean
    numeric_cols <- sapply(df, is.numeric)

    df <- df%>%
      mutate_if(is.numeric, ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))

    #For categorical columns----

    categorical_cols <- sapply(df, function(x) is.character(x) || is.factor(x))

    # Impute missing values in categorical columns with 'Unknown'
    df[, categorical_cols] <- lapply(df[, categorical_cols], function(x) {
      if (any(is.na(x))) {
        x <- replace(x, is.na(x), 'Unknown')
      }
      x
    })

    has_missing_values <- any(is.na(df))
    print(has_missing_values)
    ```

### Q4. Use the cor() function to produce a correlation matrix of the numeric values.

A4. We've created a correlation matrix using `cor() for numeric values`

```{r}
# Extract numeric columns from the dataset
numeric_columns <- df[, sapply(df, is.numeric)]

# Create a correlation matrix
correlation_matrix <- cor(numeric_columns)

```

### Q5. Produce a correlation matrix plot, and explain how to interpret it. (hint - check the corrplot or ggcorrplot plot libraries)

A5. We've plotted a correlation matrix using `corrplot()`.

```{r}

corrplot(correlation_matrix, 
         method = "color",
         type = "upper",
         order = "hclust",
         tl.col = "black",
         tl.srt = 45,
         tl.cex = 0.5,
         addrect = 3)


```

-   **How to Interpret Specific Features of This Heatmap: (Statology,
    2020)**

    -   **Symmetry**: The matrix is symmetrical around the diagonal,
        meaning the correlation between **`X`** and **`Y`** is the same
        as between **`Y`** and **`X`**, so the matrix can be read either
        above or below the diagonal line.

    -   **Strong Correlations**: Look for squares that are colored in
        dark blue or dark red to identify pairs of variables that have
        strong positive or negative correlations. These can suggest
        relationships that might be worth investigating further in your
        analysis.

    -   **Weak/No Correlations**: Cells that are white or light in color
        indicate pairs of variables where there is a weak or no
        correlation.

    -   **Potential Multicollinearity**: In the context of regression
        analysis, if two independent variables are highly correlated
        (dark blue), it could indicate multicollinearity, which might
        affect the performance of the model. It might be necessary to
        consider removing or combining these variables to mitigate the
        multicollinearity.

    -   **Target Variable**: When analyzing a correlation matrix with
        the goal of predictive modeling, we often most interested in the
        column/row corresponding to the target variable (often
        **`SalePrice`** in real estate datasets) to see which features
        are most strongly correlated with the target.(OpenAI, 2023)

    In summary, this heatmap gives a quick visual overview of how all
    variables are related to each other, which can be very useful in the
    early stages of data analysis for identifying patterns, trends, and
    relationships within the data.

### Q6. Make a scatter plot for the X continuous variable with the highest correlation with SalePrice. Do the same for the X variable with the lowest correlation with SalePrice. Finally, make a scatter plot between X and SalePrice with the correlation closest to 0.5. Interpret the scatter plots and describe how the patterns differ.

A6. We'll extract the variables with **highest**, **lowest** and
**closest** correlation to **0.5** with the variable "***SalePrice***".

```{r}
# Find the variable with the highest correlation
highest_corr_var <- names(sort(correlation_matrix["SalePrice", ], decreasing = TRUE)[2])
cat("Highest correlation with SalePrice is", highest_corr_var, "\n")

# Find the variable with the lowest correlation
lowest_corr_var <- names(which.min(correlation_matrix["SalePrice", ]))

cat("Lowest correlation with SalePrice is", lowest_corr_var, "\n")

# Find the variable with correlation closest to 0.5
closest_corr_var <- names(which.min(abs(correlation_matrix[,"SalePrice"] - 0.5)))

cat("Correlation closest to 0.5 with SalePrice is", closest_corr_var, "\n")

```

```{r}
# Scatter plot for the highest correlated variable with SalePrice
ggplot(df, aes_string(x = highest_corr_var, y = "SalePrice")) +
  geom_point() +
  ggtitle(paste("Scatter Plot for", highest_corr_var, "vs SalePrice"))


```

**SalePrice vs Overall.Qual:**

-   This plot shows a defined positive correlation between the overall
    quality of the house and the sale price.

-   As the overall quality rating increases, the sale price tends to
    increase as well, indicating that quality is an important factor in
    determining sale price.

-   The range of sale prices widens with higher quality ratings,
    implying that higher-quality houses can vary more significantly in
    price, possibly due to additional features or premium finishes not
    captured by the quality rating alone.

```{r}
# Scatter plot for the variable with the lowest correlation with SalePrice
ggplot(df, aes_string(x = lowest_corr_var, y = "SalePrice")) +
  geom_point() +
  ggtitle(paste("Scatter Plot for", lowest_corr_var, "vs SalePrice"))

```

**SalePrice vs Enclosed.Porch**:

-   This scatter plot shows a weak to no apparent correlation between
    the size of the enclosed porch and the sale price.

-   Most data points are clustered at the lower end of the enclosed
    porch size, suggesting that many houses do not have a large enclosed
    porch.

-   There's a wide spread in sale prices for houses with small to no
    enclosed porches, indicating that the enclosed porch size is not a
    strong predictor of sale price.

```{r}
# Scatter plot for the variable with correlation closest to 0.5 with SalePrice
ggplot(df, aes_string(x = closest_corr_var, y = "SalePrice")) +
  geom_point() +
  ggtitle(paste("Scatter Plot for", closest_corr_var, "vs SalePrice"))
```

**SalePrice vs TotRms.AbvGrd**:

-   There is a positive correlation between the total number of rooms
    above ground and the sale price, although it's not strictly linear.

-   As the number of rooms increases, the range of sale prices also
    increases, suggesting that larger homes (with more rooms) have a
    higher variability in sale price.

-   The vertical spread of points at each room count indicates
    variability in sale prices that could be attributed to other factors
    like location, size, or quality of the rooms.

**Differences in Patterns**:

-   The plot for 'Overall.Qual' indicates a stronger relationship with
    'SalePrice' than the other two variables. This suggests that quality
    is a stronger predictor of sale price.

-   The 'TotRms.AbvGrd' scatter plot also shows a trend where more rooms
    may lead to a higher price, but the relationship isn't as strong as
    with 'Overall.Qual'.

-   The 'Enclosed.Porch' scatter plot does not show a clear trend,
    suggesting that this feature may not have a significant impact on
    the sale price compared to the number of rooms or the overall
    quality.

-   In summary, these scatter plots suggest that house quality and size
    (as indicated by the number of rooms) are more influential on the
    sale price than the presence of an enclosed porch. The
    'Overall.Qual' plot indicates that higher quality correlates with
    higher sale prices, while 'TotRms.AbvGrd' suggests that larger homes
    with more rooms tend to sell for more. The 'Enclosed.Porch' plot
    does not show a significant relationship with sale price, indicating
    it may not be an important feature for predicting housing prices in
    this dataset.

### Q7. Fit a regression model in R, using at least 3 continuous variables.

A7. First, we'll identify variables with high correlation values with
respect to "***SalePrice***"

```{r}
# Identify the top 4 correlated variables with SalePrice
top_cor_vars <- names(sort(correlation_matrix["SalePrice", ], decreasing = TRUE)[2:5])
print(top_cor_vars)

```

-   Now, we'll fit a **Linear regression model** using the variables
    obtained above

```{r}
regression_model <- lm(SalePrice ~ ., data = df[, c("SalePrice", top_cor_vars)])

summary(regression_model)
```

### Q8. Report the model in equation form and interpret each coefficient of the model in the context of this problem.

A8. I have fitted a regression model to predict "***SalePrice***" based
on several variables. The equation of the model is as follows:

### Equation form:

SalePrice= −103,800+28,000×Overall Qual + 50.86×Gr Liv Area +
5,585×Garage Cars + 58.86×Garage Area

Now, let me interpret each coefficient in the context of the problem:

1.  **Intercept (-103,800):**

    -   The intercept represents the estimated SalePrice when all other
        predictor variables are zero. However, in our housing price
        prediction context, this might not have a meaningful
        interpretation.

2.  **Overall Quality (28,000):**

    -   For each unit increase in Overall Quality, I expect the
        SalePrice to increase by \$28,000, holding other variables
        constant.

    -   Pr(\>\|t\|): \< 2e-16 (Highly statistically significant)

3.  **Ground Living Area (50.86):**

    -   For each additional square foot increase in Ground Living Area,
        I expect the SalePrice to increase by \$50.86, holding other
        variables constant.

    -   Pr(\>\|t\|): \< 2e-16 (Highly statistically significant)

4.  **Garage Cars (5,585):**

    -   For each additional car capacity in the garage, I expect the
        SalePrice to increase by \$5,585, holding other variables
        constant.

    -   Pr(\>\|t\|): 0.01 (Statistically significant)

5.  **Garage Area (58.86):**

    -   For each additional square foot increase in Garage Area, I
        expect the SalePrice to increase by \$58.86, holding other
        variables constant.

    -   Pr(\>\|t\|): 4.1e-15 (Highly statistically significant)

In summary, all variables in the model have statistically significant
coefficients. The ***"Overall.Qual," "Gr.Liv.Area," and*** "Garage.Area"
variables are highly significant, while ***"Garage.Cars"*** is
statistically significant but less so compared to the others. The
Adjusted R sqaured value of 0.7583 indicates that approximately 75.83%
of the variability in SalePrice can be explained by the model and
indicates a good fit.

### Q9. Use the plot() function to plot your regression model. Interpret the four graphs that are produced.

A9. We've plotted 4 graphs using `plot()` and have interpreted the same.
("4.2 - Residuals Vs. Fits Plot \| STAT 501," n.d.)

```{r}
plot(regression_model)
```

**Interpretation**:

1.  **Residuals vs Leverage Plot**

    This plot helps to find influential cases (if any) in the regression
    model. The points far from zero on the x-axis (leverage) are points
    that have more "influence" on the regression equation. The Cook's
    distance lines (dashed lines) help you identify these influential
    points. Points that are outside the dashed lines are considered to
    have high leverage.

2.  **Scale-Location Plot**

    This plot shows how residuals are spread along the range of
    predictors. It's used to check the assumption of equal variance
    (homoscedasticity). Ideally, the red line should be approximately
    horizontal at the value of 1 on the y-axis.

3.  **Q-Q Plot (Quantile-Quantile Plot)**

    This plot shows if the residuals are normally distributed. Points
    following the dashed line suggest normality. Deviations from this
    line indicate departures from normality.

4.  **Residuals vs Fitted Plot**

    This plot is used to check the assumption that the residuals do not
    have any systematic relationship with the fitted values, which would
    indicate non-linearity in the data. A horizontal line with randomly
    spread points around it suggests that the assumption is true.

In each of these plots, we see one or more points labeled, usually with
an observation number. These are points that the diagnostic measures
(like Cook's distance) have identified as being potentially influential
or outliers, and they warrant further investigation.

For instance, in the Residuals vs Leverage plot, point 14990 has high
leverage, and in the Scale-Location and Residuals vs Fitted plots,
points 14990, 21810, and 21820 might be potential outliers or
influential points due to their placement on the graph. In the Q-Q plot,
the deviations of the points from the line at the ends suggest potential
issues with the normality of the residuals.

In summary, these plots are used to check for assumptions and potential
problems in the regression model such as **non-linearity, non-constant
variance, outliers, and influential data points**. The data points
labeled in these plots are the ones that the model suggests should be
looked at more closely.

### Q10. Check your model for multicollinearity and report your findings. What steps would you take to correct multicollinearity if it exists?

A10. We'll use **Variance Inflation Factor (VIF)** for checking
**multicollinearity.** (Team, 2023)

```{r}
# Check for multicollinearity using VIF
vif_results <- vif(regression_model)

# Print VIF values
print(vif_results)
```

\
**Variance Inflation Factor (VIF)** is a measure that quantifies how
much the variance of the estimated regression coefficients increases
when your predictors are correlated. The interpretation of VIF values is
as follows:

-   **VIF = 1:**

    -   No multicollinearity. This indicates that the predictor is not
        correlated with other predictors.

-   **VIF \> 1 but \< 5:**

    -   Moderate multicollinearity. This suggests some correlation with
        other predictors. This is generally acceptable.

-   **VIF \> 5:**

    -   High multicollinearity. This indicates a problematic level of
        correlation with other predictors. This may lead to unreliable
        coefficient estimates.

**Interpretation:**

-   Overall, Overall Quality and Ground Living Area have VIF values
    close to 1, indicating low multicollinearity.

-   Garage.Area has VIF = 4.8 indicating moderate multicollinearity

-   Garage.Cars has VIF = 5.15 indicating high multicollinearity

**Corrective Steps:**

-   Removing variables with High **VIF**

-   Removing Highly correlated Predictors

-   Contextual consideration - In some cases, multicollinearity may not
    be a significant issue, especially if the goal is prediction rather
    than interpretation.

-   I would remove Garage.Cars from the regression model due to high
    multicollinearity

### Q11. Check your model for outliers and report your findings. Should these observations be removed from the model?

A11. In order to check for model outliers, we'll do **Residuals
analysis** and **Residuals vs Leverage analysis**

```{r}
# Assuming your model is named as 'regression_model'
plot(regression_model, which = 1) # Residuals vs Fitted
plot(regression_model, which = 5) # Cook's Distance Plot



```

**Interpretation:**

1.  **Residuals vs Fitted**

    In the above plot, there are points on the right that have much
    larger residuals compared to others. These are potential outliers,
    particularly those with very high SalePrice values which are not
    well predicted by the model. The points are also labeled with their
    observation numbers (e.g., 2182, 2181, 1499), making it easy to
    identify them in your dataset.

2.  **Residuals vs Leverage**

    The Residuals vs. Leverage plot indicates that observations 2182,
    2181, and 1499 are potential outliers or influential points due to
    their high leverage and/or large residuals, as they lie beyond the
    Cook's distance threshold lines.

We'll remove outliers based on cook's distance which is the dotted line
in the above graph.

```{r}
# Calculating Cook's Distance
cooks.distance <- cooks.distance(regression_model)

# Identifying potential outliers
potential_outliers <- which(cooks.distance > 4/(nrow(df)-length(coef(regression_model))))



```

### **Decision on Removing Outliers:**

-   **Not Always Necessary:** Removing outliers is not always necessary.
    Sometimes, they represent valuable extremes in your data.

-   **Context Matters:** The decision should be based on the context. If
    outliers represent errors or anomalies not relevant to the analysis,
    they can be removed.

-   **Impact Assessment:** Assess the impact of outliers on model
    performance. If their inclusion significantly distorts the model,
    consider removing them.

    (OpenAI, 2023)

### Q12. Attempt to correct any issues that you have discovered in your model. Did your changes improve the model? Why or why not?

A.12

**Issues discovered in the model:**

1.  Garage.Cars has VIF \> 5, indicating high multicollinearity.
2.  Garage.Cars has is statistically significant but not as much as the
    other variables.
3.  Potential outliers have been detected using Cook's distance

**Steps to take:**

1.  Remove Garage.Cars from the model
2.  Remove the outliers and check if model performs any better

```{r}
df_clean <- df[-potential_outliers, ]

new_vars <- select(df_clean,"Overall.Qual", "Gr.Liv.Area","Garage.Area", "SalePrice")

regression_model <- lm(SalePrice ~ ., data = new_vars)

summary(regression_model)



```

**Interpretation:**

1.  **Lower Residual Standard Error:** The residual standard error in
    the new model is 27,230, which is lower than the 39,280 in the old
    model. A lower residual standard error indicates that the new
    model's predictions are, on average, closer to the actual values,
    which is a sign of improved model fit.

2.  **Higher Multiple R-squared**: The multiple R-squared in the new
    model is 0.818, whereas it was 0.7586 in the old model. This
    suggests that the new model explains the variance in SalePrice more
    effectively.

3.  **Adjusted R-squared**: The adjusted R-squared is also higher in the
    new model (0.8178) compared to the old model (0.7583). A higher
    adjusted R-squared indicates a better balance between model
    complexity and explanatory power. (OpenAI, 2023)

4.  **F-statistic**: The F-statistic in the new model (4133) is
    considerably higher than in the old model (2298). The F-statistic
    tests the overall significance of the model, and a higher value
    suggests that the new model is a better fit for the data.

5.  **Coefficient Interpretation**: While the coefficients for
    "Overall.Qual," "Gr.Liv.Area," and "Garage.Area" are similar between
    the old and new models, the overall improvement in model fit
    indicates that the new model is a better representation of the
    relationship between these variables and SalePrice.

In summary, the new model has a lower residual standard error, higher
R-squared values, and a higher F-statistic compared to the old model.
These improvements indicate that the new model is a better fit for the
data and provides more accurate predictions of ***SalePrice.***

### Q13. Identify the "best" model using the all-subsets regression method. State the preferred model in equation form.

A13. We'll perform all-subsets regression to find out which subset of
predictor variables gives us the best accuracy. (OpenAI, 2023)

```{r}

# Define predictors
predictors <- c("Overall.Qual", "Gr.Liv.Area", "Garage.Area")
# Subset the relevant predictors and response variable
data_subset <- subset(df_clean, select = c("SalePrice", predictors))
# Generate all subsets of the predictors up to 3 variables
models <- regsubsets(SalePrice ~ ., data = data_subset, nvmax = 3)

summary(models)
# Plot a table of models showing variables in each model ordered by the selection statistic (adjusted R2)
plot(models, scale = "adjr2")


```

**Interpretation:**

-   **Bars**: Each bar represents a model with a different combination
    of predictors. The length of the bar indicates the value of the
    adjusted R-squared for that model.

-   **Black and Grey Scale**: The filled part of the bar (in black or
    grey) indicates the predictors that are included in the model. A
    filled block under a predictor name means that the predictor is
    included in the model corresponding to that bar.

-   **Adjusted R-squared (adjr2)**: The adjusted R-squared is a modified
    version of R-squared that has been adjusted for the number of
    predictors in the model. It provides a measure of how well the model
    fits the data while penalizing for the number of predictors.

-   **(Intercept)**: This is the baseline of the model. It is always
    included since it represents the constant term in the regression
    equation.

**Interpreting the graph above, it appears that:**

-   The model with the **highest adjusted R-squared** includes all three
    predictors: **`Overall.Qual`**, **`Gr.Liv.Area`**, and
    **`Garage.Area`**.

-   The black bar reaches the highest on the scale of adjusted
    R-squared, suggesting that the model with all three predictors has
    the best fit among the models considered, after adjusting for the
    number of predictors.

-   The length of the bars decreases as the number of predictors
    decreases, indicating that models with fewer predictors have lower
    adjusted R-squared values.

In conclusion, based on the adjusted R-squared values, the model with
**`Overall.Qual`**, **`Gr.Liv.Area`**, and **`Garage.Area`** is
preferred because it has the highest adjusted R-squared, suggesting it
provides the best balance between model complexity (number of
predictors) and fit.

### Equation form of preferred model :

SalePrice= −81275.833+24160.729×Overall.Qual + 51.141×Gr.Liv.Area +
74.945×Garage.Area

### Q14. Compare the preferred model from step 13 with your model from step 12. How do they differ? Which model do you prefer and why?

A14. The preferred model in Ques13 and model selected in Ques12 are the
same. Both the models have 3 predictor variables and give the best
Adjusted R-squared value when all the 3 variables are selected together.

# Conclusion

Our analysis of the Ames Housing dataset led to an understanding of the
housing market factors. Our final model, based on Overall Quality,
Ground Living Area, and Garage Area, accounted for approximately 81.8%
of the variance in SalePrice, indicating a strong model fit. The
all-subsets regression method confirmed these three variables as the
optimal predictors. This model outperformed the initial models, which
included variables that exhibited high multicollinearity. Upon comparing
the models, we observed that removing Garage Cars and addressing
outliers improved the predictive power and accuracy of our model. The
preferred model not only provides a quantifiable measure of house
valuation but also reflects the significant impact of a house's quality,
size, and garage area on its sale price. This model can serve as a
robust tool for potential buyers, sellers, and investors in the Ames
housing market.

# References

1.  OpenAI. (2021). ChatGPT (Version 3.5).
    OpenAI.<https://chat.openai.com/>

2.  Z. (2023, February 8). How to Read a Correlation Matrix. Retrieved
    from <https://www.statology.org/how-to-read-a-correlation-matrix/>

3.  4.2 - Residuals vs. Fits Plot \| STAT 501. (n.d.). Retrieved from
    <https://online.stat.psu.edu/stat501/lesson/4/4.2>

4.  Team, I. (2023, October 1). Variance Inflation Factor (VIF).
    Retrieved from
    <https://www.investopedia.com/terms/v/variance-inflation-factor.asp>
